{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4550eca6-5b20-41c9-99c8-0e07e4a6f731",
   "metadata": {},
   "source": [
    "## Understanding inner function of NN through Logistic Regression Binary Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591582ee-477e-427d-b4d2-1797952d448d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66b01b9-51ce-47f8-94d2-1ebab00a038a",
   "metadata": {},
   "source": [
    "### 1. read files from directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108b6551-421f-460e-a891-9c4e48a33cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = h5py.File('train_catvnoncat.h5', \"r\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af336fd2-8bbc-4103-8d15-109c71188c24",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = h5py.File('test_catvnoncat.h5', \"r\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba4c84-3e0c-48e6-b48d-81d00e045c77",
   "metadata": {},
   "source": [
    "**Exercise 1:**\n",
    "Find the values for:\n",
    "\n",
    "m_train (number of training examples)\n",
    "m_test (number of test examples)\n",
    "num_px (= height = width of a training image) Remember that train_set_x_orig is a numpy-array of shape (m_train, num_px, num_px, 3). For instance, you can access m_train by writing train_set_x_orig.shape[0]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a105c2-f6cf-4bbe-a64f-74ca79bd3009",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfb20b38-d33d-45ab-9b0e-c49b3aa75ff2",
   "metadata": {},
   "source": [
    "### Get the training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628830aa-a7e9-44f2-bcb6-221fb15e5ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x_orig = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n",
    "train_set_y_orig = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n",
    "\n",
    "test_set_x_orig = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n",
    "test_set_y_orig = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n",
    "\n",
    "classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa5dba25-2344-4bc8-8718-12759e58fd7c",
   "metadata": {},
   "source": [
    "### Plot an image from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f2ecb0-043d-4f21-b48f-a69c2d47df93",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36fefb22-9d66-4cff-8ad4-fa0a98922cc6",
   "metadata": {},
   "source": [
    "### 2. Reshape/flatten a matrix\n",
    "**Exercise 2:**\n",
    "\n",
    "Reshape the training and test data sets so that images of size (num_px, num_px, 3) are flattened into single vectors of shape (num_px, num_px, 3, 1).\n",
    "\n",
    "A trick when you want to flatten a matrix X of shape (a,b,c,d) to a matrix X_flatten of shape ($b*c*d$, a) is to use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ebed79-4cd6-435b-a314-a5aa1f84473d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "88710e43-2d14-41d3-8f1f-4f80204e387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"train_set_x_flatten shape: \" + str(train_set_x_flatten.shape))\n",
    "#print (\"train_set_y shape: \" + str(train_set_y.shape))\n",
    "print (\"test_set_x_flatten shape: \" + str(test_set_x_flatten.shape))\n",
    "#print (\"test_set_y shape: \" + str(test_set_y.shape))\n",
    "print (\"sanity check after reshaping: \" + str(train_set_x_flatten[0:5,0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "464e20c7-3cf1-4ffb-a91b-49c35ec7ddc2",
   "metadata": {},
   "source": [
    "### 3. Scale the dataset\n",
    "To represent color images, the red, green and blue channels (RGB) must be specified for each pixel, and so the pixel value is actually a vector of three numbers ranging from 0 to 255.\n",
    "\n",
    "One common preprocessing step in machine learning is to center and standardize your dataset, meaning that you substract the mean of the whole numpy array from each example, and then divide each example by the standard deviation of the whole numpy array. But for picture datasets, it is simpler and more convenient and works almost as well to just divide every row of the dataset by 255 (the maximum value of a pixel channel)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c48fc0-922c-4921-a64d-7bb97250aa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_x = train_set_x_flatten/255.\n",
    "test_set_x = test_set_x_flatten/255."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e46ef53a-e2c8-4ecb-a430-f221ffa491a4",
   "metadata": {},
   "source": [
    "### 4 - Building the parts of our algorithm\n",
    "The main steps for building a Neural Network are:\n",
    "\n",
    "1. Define the model structure (such as number of input features)\n",
    "2. Initialize the model's parameters\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "\n",
    "You often build 1-3 separately and integrate them into one function we call model().\n",
    "\n",
    "### 4.1 Helper function- Sigmoid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80b07a9-00c0-4e40-a362-a19c49e55098",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b8c81f3c-e683-4073-9b1a-3c731f308931",
   "metadata": {},
   "source": [
    "### 4.2 Initializing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021509af-e9b3-4e8a-8ac0-d5cd5a04705d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_with_zeros(dim):\n",
    "    \"\"\"\n",
    "    This function creates a vector of zeros of shape (dim, 1) for w and initializes b to 0.\n",
    "    \n",
    "    Argument:\n",
    "    dim -- size of the w vector we want (or number of parameters in this case)\n",
    "    \n",
    "    Returns:\n",
    "    w -- initialized vector of shape (dim, 1)\n",
    "    b -- initialized scalar (corresponds to the bias)\n",
    "    \"\"\"\n",
    "\n",
    "    b#code here\n",
    "\n",
    "    #####\n",
    "\n",
    "    #check the dimension\n",
    "    assert(w.shape == (dim, 1) )\n",
    "    assert(isinstance(b, float) or isinstance(b, int))\n",
    "\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6a71a-9c3c-445d-94bc-2c29813c41f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#verify\n",
    "w, b = initialize_with_zeros(3)\n",
    "print(\"w \" + str(w))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635c28d1-1f97-472c-8f27-8028d26004fd",
   "metadata": {},
   "source": [
    "### 4.3 FP and BP\n",
    "\n",
    "**Forward Propagation:**\n",
    "- Get X,\n",
    "- Compute $A = \\sigma(w^T X + b) = (a^{(0)}, a^{(1)}, ..., a^{(m-1)}, a^{(m)})$,\n",
    "- Cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\"\n",
    "  \n",
    "**Back Propagation:** \n",
    "    Here are the two formulas you will be using:\"\n",
    "\n",
    "- $\\frac{\\partial J}{\\partial w} = \\frac{1}{m}X(A-Y)^T\\tag{7}$,\n",
    "- $ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{8}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df479834-b2a8-4a1d-a012-51ef9d730a91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the cost function and its gradient for the propagation explained above\n",
    "\n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat) of size (1, number of examples)\n",
    "\n",
    "    Return:\n",
    "    cost -- negative log-likelihood cost for logistic regression\n",
    "    dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "    db -- gradient of the loss with respect to b, thus same shape as b\n",
    "    \n",
    "    Tips:\n",
    "    - Write your code step by step for the propagation. np.log(), np.dot()\n",
    "    \"\"\"\n",
    "    #no of examples\n",
    "    m = X.shape[1]\n",
    "\n",
    "    #Forward Propagation   (CODE HERE - 2 lines) \n",
    "    A = \n",
    "    J = \n",
    "      \n",
    "\n",
    "    # Back Propagation (CODE HERE - 2 lines) \n",
    "    dw = \n",
    "    db = \n",
    "\n",
    "\n",
    "    #dimension check\n",
    "    assert(dw.shape == w.shape)\n",
    "    assert(db.dtype == float)\n",
    "\n",
    "    grads = {\"dw\": dw,\n",
    "             \"db\": db}\n",
    "\n",
    "    return grads, J\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984bd4d9-9c15-4ef1-9937-befdc20cfab5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check the function\n",
    "w, b, X, Y = np.array([[1.],[2.]]), 2., np.array([[1.,2.,-1.],[3.,4.,-3.2]]), np.array([[1,0,1]])\n",
    "grads, cost = propagate(w, b, X, Y)\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))\n",
    "print (\"cost = \" + str(cost))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c192459b-808a-4bed-aace-75fad3c4af45",
   "metadata": {},
   "source": [
    "```\n",
    "dw = [[0.99845601] [2.39507239]]\n",
    " ```\n",
    " \n",
    "db = 0.001455578136784208\n",
    "cost = 5.801545319394553"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64816b2-7edc-4a97-9ae4-3269e022b999",
   "metadata": {},
   "source": [
    "### 4.4 Optimization\n",
    "- You have initialized your parameters.\n",
    "- You are also able to compute a cost function and its gradient.\n",
    "- Now, you want to update the parameters using gradient descent.\n",
    "  \n",
    "**Exercise:**\n",
    "  Write down the optimization function. The goal is to learn $w$ and $b$ by minimising the cost function $J$. For a parameter $\\theta$ , the update rule is  $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\n",
    "\\alpha$ is the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554ab096-70d0-4b87-8ddc-0001237fe4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \"\"\"\n",
    "    This function optimizes w and b by running a gradient descent algorithm\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of shape (num_px * num_px * 3, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat), of shape (1, number of examples)\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    print_cost -- True to print the loss every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    params -- dictionary containing the weights w and bias b\n",
    "    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "    costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve.\n",
    "    \n",
    "    Tips:\n",
    "    You basically need to write down two steps and iterate through them:\n",
    "        1) Calculate the cost and the gradient for the current parameters. Use propagate().\n",
    "        2) Update the parameters using gradient descent rule for w and b.\n",
    "    \"\"\"\n",
    "    #store all the cost values for each iteration\n",
    "    costs = []\n",
    "\n",
    "    print(\"dimensions : w\", w.shape, \"X :\", X.shape, \"Y: \", Y.shape) \n",
    "    \n",
    "    # loop over no of iterations\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        #calculate the cost and grads for the initial guess of w, b \n",
    "        #CODE here (1 line) \n",
    "        \n",
    "\n",
    "        #store\n",
    "        if (i%100 == 0):\n",
    "            costs.append(J)\n",
    "\n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, J))\n",
    "\n",
    "        # extract the gradients and update the w, b\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "\n",
    "        #code here( 2 lines)\n",
    "        w = \n",
    "        b = \n",
    "\n",
    "    params = {\"w\": w, \"b\":b}\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "\n",
    "    return params, grads, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4b2d8-9348-4148-a649-7bebc8bfb6c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#check\n",
    "params, grads, costs = optimize(w, b, X, Y, num_iterations= 100, learning_rate = 0.009, print_cost = True)\n",
    "\n",
    "print (\"w = \" + str(params[\"w\"]))\n",
    "print (\"b = \" + str(params[\"b\"]))\n",
    "print (\"dw = \" + str(grads[\"dw\"]))\n",
    "print (\"db = \" + str(grads[\"db\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a64442d-952e-4f85-8a42-407e5cf27175",
   "metadata": {},
   "source": [
    "Expected output:\n",
    "```w = [[0.19033591]\n",
    " [0.12259159]]\n",
    "b = 1.9253598300845747\n",
    "dw = [[0.67752042]\n",
    " [1.41625495]]\n",
    "db = 0.21919450454067657\n",
    "```\n",
    "\n",
    "**Exercise:**\n",
    "The previous function will output the learned $w$ and $b$. We are able to use $w$ and $b$ to predict the labels for a dataset X. Implement the predict() function. There is two steps to computing predictions:\n",
    "\n",
    "- Calculate ${\\hat Y} = A = \\sigma(w^T X + b)$\n",
    "\n",
    "- Convert the entries of a into 0 (if activation <= 0.5) or 1 (if activation > 0.5), stores the predictions in a vector $Y_{prediction}$. If you wish, you can use an `if/else` statement in a `for` loop (though there is also a way to vectorize this)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9183e0ac-73be-4c28-a9c2-3306ff078f65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: predict\n",
    "\n",
    "def predict(w, b, X):\n",
    "    '''\n",
    "    Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "    \n",
    "    Arguments:\n",
    "    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n",
    "    b -- bias, a scalar\n",
    "    X -- data of size (num_px * num_px * 3, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    Y_prediction -- a numpy array (vector) containing all predictions (0/1) for the examples in X\n",
    "    '''\n",
    "   \n",
    "    #apply sigmoid (CODE HERE 1 line)\n",
    "    A = \n",
    "    print(\"A shape\", A.shape, \" X shape\", X.shape)\n",
    "\n",
    "    #convert the output into output class 0/1 for all the training examples\n",
    "\n",
    "    #no of examples\n",
    "    m = X.shape[1]\n",
    "    print(\"sample size =\", m, \"and no of features =\", X.shape[0], \"w shape: \" , w.shape )\n",
    "\n",
    "    #initialize Y_pred\n",
    "    Y_pred = np.zeros((1, m))\n",
    "\n",
    "    # Assign 0 or 1 for Y_pred variable CODE HERE (2 lines) \n",
    "\n",
    "\n",
    "\n",
    "    #check the dimension\n",
    "    assert(Y_pred.shape == (1, m))\n",
    "\n",
    "    print(\"*****prediction completed ***********\")\n",
    "    \n",
    "    return Y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d8e740c-79d0-490e-8887-488be4d6ed8b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
